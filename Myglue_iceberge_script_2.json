{
	"jobConfig": {
		"name": "myjobdemo2",
		"description": "",
		"role": "arn:aws:iam::116981782851:role/service-role/AWSGlueServiceRole-NEW",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 6,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "myjobdemo2.py",
		"scriptLocation": "s3://aws-glue-assets-116981782851-ap-south-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--datalake-formats",
				"value": "iceberg",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-11-21T19:06:01.516Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-116981782851-ap-south-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-116981782851-ap-south-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import *\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\n\r\n# S3 and Glue Catalog Setup\r\nwarehouse_path = 's3://my-bucket-ice/iceberg_data/'\r\ncatalog_nm = 'glue_catalog'\r\n\r\n# Initialize Spark Session with Iceberg Catalog configuration\r\nspark = SparkSession.builder \\\r\n    .config(f\"spark.sql.catalog.{catalog_nm}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\r\n    .config(f\"spark.sql.catalog.{catalog_nm}.warehouse\", warehouse_path) \\\r\n    .config(f\"spark.sql.catalog.{catalog_nm}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\r\n    .config(f\"spark.sql.catalog.{catalog_nm}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\r\n    .config(f\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\r\n    .getOrCreate()\r\n\r\n# AWS Glue Job Arguments\r\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\r\n\r\n# Initialize GlueContext with the SparkContext\r\nibsc = spark.sparkContext\r\nibglueContext = GlueContext(ibsc)\r\nibjob = Job(ibglueContext)\r\nibjob.init(args[\"JOB_NAME\"], args)\r\n\r\n# Input and Output Table Information\r\nin_database = \"my_database\"\r\nin_table_name = \"csv_table\"\r\ndatabase_op = 'my_database'\r\ntable_op = 'iceberg_table'\r\n\r\nprint(f\"\\nINPUT Database : {in_database}\")\r\nprint(f\"\\nINPUT Table : {in_table_name}\")\r\nprint(f\"\\nOUTPUT Iceberg Database : {database_op}\")\r\nprint(f\"\\nOUTPUT Iceberg Table : {table_op}\")\r\n\r\n# Read the Glue input table as a DynamicFrame\r\nInputDynamicFrameTable = ibglueContext.create_dynamic_frame.from_catalog(database=in_database, table_name=in_table_name)\r\n\r\n# Convert the DynamicFrame to a Spark DataFrame\r\nInputDynamicFrameTable_DF = InputDynamicFrameTable.toDF()\r\n\r\n# Register the DataFrame as a TempView\r\nInputDynamicFrameTable_DF.createOrReplaceTempView(\"InputDataFrameTable\")\r\nspark.sql(\"SELECT * FROM InputDataFrameTable LIMIT 10\").show()\r\n\r\n# Filter the DataFrame for rows where age < 30\r\ncolname_df = spark.sql(\"SELECT * FROM InputDataFrameTable WHERE age < 30\")\r\ncolname_df.createOrReplaceTempView(\"OutputDataFrameTable\")\r\n\r\n# Write the filtered data to an Iceberg table\r\nib_Write_SQL = f\"\"\"\r\n    CREATE OR REPLACE TABLE {catalog_nm}.{database_op}.{table_op}\r\n    USING iceberg\r\n    TBLPROPERTIES (\"format-version\"=\"2\", \"write_compression\"=\"gzip\")\r\n    AS SELECT * FROM OutputDataFrameTable;\r\n    \"\"\"\r\n\r\n# Run the Spark SQL query\r\nspark.sql(ib_Write_SQL)\r\n\r\nspark.sql(f\"SELECT * FROM {catalog_nm}.`{database_op}`.{table_op}\")\r\n\r\nupdate_sql = f\"\"\"\r\n    UPDATE {catalog_nm}.{database_op}.{table_op}\r\n    SET age = 55\r\n    WHERE id = 6\r\n\"\"\"\r\n\r\n# Execute the update command\r\nspark.sql(update_sql)\r\n\r\nspark.sql(f\"\"\" ALTER TABLE {catalog_nm}.{database_op}.{table_op} ADD COLUMN department STRING \"\"\")\r\n\r\nhistory_df = spark.sql(f\"\"\"\r\nSELECT * FROM {catalog_nm}.{database_op}.{table_op}.history LIMIT 5\r\n\"\"\")\r\nhistory_df.show()\r\n\r\n# Fetch the latest snapshot ID\r\nlatest_snapshot_df = spark.sql(f\"SELECT snapshot_id FROM {catalog_nm}.{database_op}.{table_op}.history ORDER BY snapshot_id DESC LIMIT 1\")\r\nlatest_snapshot_id = latest_snapshot_df.collect()[0]['snapshot_id']\r\nprint(latest_snapshot_id)\r\n\r\n# Perform the time travel query using the latest snapshot ID\r\nlatest_snapshot_query = f\"\"\"\r\n    SELECT * FROM {catalog_nm}.{database_op}.{table_op} FOR VERSION AS OF {latest_snapshot_id}\r\n\"\"\"\r\nspark.sql(latest_snapshot_query).show()\r\n\r\n\r\n###time travel\r\n\r\n# Fetch the latest timestamp\r\nlatest_timestamp_df = spark.sql(f\"SELECT made_current_at FROM {catalog_nm}.{database_op}.{table_op}.history ORDER BY made_current_at DESC LIMIT 1\")\r\nlatest_timestamp = latest_timestamp_df.collect()[0]['made_current_at']\r\n\r\nprint(latest_timestamp)\r\n\r\n\r\n# Perform the time travel query using the latest timestamp\r\ntime_travel_timestamp_query = f\"\"\"\r\n    SELECT * FROM {catalog_nm}.{database_op}.{table_op} FOR SYSTEM_TIME AS OF '{latest_timestamp}'\r\n\"\"\"\r\nspark.sql(time_travel_timestamp_query).show()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
}